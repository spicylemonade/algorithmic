The `research_rubric.json` has been written and validated. Here's a summary of the rubric:

**26 items across 5 phases:**

| Phase | Items | Focus |
|-------|-------|-------|
| **Phase 1: Problem Analysis & Literature Review** | 6 (item_001–006) | Repo analysis, web-based literature review (8 topic areas), sources.bib with 15+ papers, data source catalog, mathematical formulation, gap analysis vs. state-of-the-art |
| **Phase 2: Baseline Implementation & Metrics** | 5 (item_007–011) | Forward scattering model, convex inversion solver, sparse data handler, mesh comparison metrics, data ingestion layer |
| **Phase 3: Core Research & Novel Approaches** | 5 (item_012–016) | SAGE-style evolutionary solver, hybrid convex-to-GA pipeline, sparse-only inversion mode, bootstrap uncertainty quantification, C++ acceleration extension |
| **Phase 4: Experiments & Evaluation** | 7 (item_017–023) | Ground truth benchmark assembly, blind inversion tests, error metrics vs. ground truth, recursive optimization loop, sparse degradation stress test, top-50 candidate generation, pipeline execution on top-10 candidates |
| **Phase 5: Analysis & Documentation** | 3 (item_024–026) | Validation report with comparative analysis against prior work (MPO LCInvert, SAGE, KOALA), source code documentation + README, reproducibility packaging |

Key design decisions:
- **Phase 1** requires web search for 8 specific topic areas and mandates at least 15 papers in `sources.bib` with named required entries
- Later phases reference comparing results against prior work found in the literature (e.g., item_019 compares against DAMIT accuracies, item_021 references sparse results from Durech et al. 2010, item_024 compares against MPO LCInvert/SAGE/KOALA)
- Every item has specific, measurable acceptance criteria with numerical thresholds where applicable
- Items build logically: literature review informs implementation, implementation enables validation, validation feeds recursive optimization, which enables candidate modeling
